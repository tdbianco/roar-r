---
title: "LSE vs MM"
author: "Teresa Del Bianco"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

Here we present a comparison between the standard least-squared regression model and mixed modelling. When we want to obtain individual beta coefficients (1 for each individual), why should we do a mixed models - entering the individual as random effect), rather than 1) running a lsm with individual as a predictor 2) running a separate lsm for each subject? 

# Method

We fit 3 linear regressions:

* Non-pooled ordinary least squared regression (OLS) with the average proportional looking time (PLT) as the response variable. In order to obtain a score for each participant, we excluded the average intercept from the calculation. 

* Individual OLS with the average proportional looking time (PLT) as the response variable. Each regression included the individual participant’s data only. 

* Multilevel regression (also know as mixed model) with the average proportional looking time (PLT) as the response variable. The random effect specification allowed for correlated varying intercept and slopes per participant. 

As an example, we are going to enter a continous variable, IQ, as unique predictor, and evaluate the change in the beta coefficient of the Intercept base on standard errors and N of observations. The beta coefficient represents the unstandardised effect size of each component; the standard error represents a measure of precision around the beta coefficient. 

For each regression type, the beta coefficient and the standard errors correspond to: 

* Non-pooled OLS: the intercept corresponds to the individual estimate of the participant’s beta coefficient and its standard error. We obtained the individual estimate/error of the Intercept from the interaction between the time component and the participant level. In this case, the standard error corresponds to the square root of the sample variance, or mean squared error (MSE). The formula of the non-pooled OLS looks like: DV ~ IQ * Participant

* Individual OLS: we obtained individual estimates and standard errors of intercept; we derived the degrees of freedom from actual number of observations per participant. In this case, the standard error refers to the individual variance MSE. The formula of the non-pooled OLS looks like: for i in total(Participants), DV(i) ~ IQ(i)

* Multilevel model: we extracted the individual coefficients, corresponding to the sum of the fixed effect and the individual variance component (as specified in the random effect). In this case, we assumed as standard error the joint MSE of the summated variances of the fixed and the random effect. The formula of the non-pooled OLS looks like: DV ~ IQ + (1|Participant). 

# Linear regression

Sample ds:

```{r, echo=FALSE}
data <- read.csv("dat-hbb.csv")

library(dplyr)
data_head <- data %>%
  filter(AOI=="head") %>%
  mutate(ID=as.factor(ID))

sample_n(data_head, 5)
```

## OLS with Complete pooling

This is a standard regression that does not include individual estimates of beta:

```{r}
mod_lm_poo<-lm(prop.LT~fsiq4_all,data=data_head)
summary(mod_lm_poo)
```

## OLS with No pooling

This standard regression does include individual estimates of beta because ID is entered as a preditor.

```{r}
mod_lm_npoo<-lm(prop.LT~fsiq4_all+ID-1,data=data_head)
coef <- summary(mod_lm_npoo)$coefficients
coef.ls.npoo <- rbind(data.frame(ID=gsub(pattern="ID", replacement="", 
                                         x=rownames(coef[-1,])), 
                           Intercept=coef[-1,1],
                           St.er.int=coef[-1,2],
                           Method="ls_npoo"))
rownames(coef.ls.npoo) <- NULL
head(as.data.frame(
  summary(mod_lm_npoo)$coefficients) %>% tibble::rownames_to_column("id"))
```

## Individual r for each item

Here we run a standard regression for each subject, thus obtaining a different estimate for each individual. 

```{r}
coef.ls.ind <- data.frame()
ids <- unique(data_head$ID)
for (i in 1:length(ids)) {
  d.s = subset(data_head, ID==ids[i])
  if (is.nan(mean(d.s$fsiq4_all, na.rm=T))) 
    {
    coef.ls.ind = rbind(coef.ls.ind, data.frame(ID=(ids[i]), 
                                      Intercept=NA,
                                      St.er.int=NA,
                                      Method="ls.ind"))
    }
  
  else 
    {
  ls.fit = lm(prop.LT~fsiq4_all, data=d.s)
  s.ls.fit = summary(ls.fit)$coefficients
  coef.ls.ind = rbind(coef.ls.ind, data.frame(ID=(ids[i]), 
                                      Intercept=s.ls.fit[1,1],
                                      St.er.int=s.ls.fit[1,2],
                                      Method="ls.ind"))
  }
}
head(coef.ls.ind)
```

## Mixed approach

In the mixed model, we specify a random effect that will group the observations of 1 participant together. When the lmm is fit, each participant will have his/her own estimate fit.

```{r}
library(lmerTest)
library(sjstats)
mod_mm<-lmer(prop.LT~fsiq4_all+(1|ID),data=data_head)
coef <- coef(mod_mm)$ID$'(Intercept)'
err <- se(mod_mm)$ID$'(Intercept)'
coef.mm <- rbind(data.frame(ID=rownames(coef(mod_mm)$ID), 
                           Intercept=coef,
                           St.er.int=err,
                           Method="mm"))
head(coef.mm)
```

```{r}
all_coef <- rbind(coef.ls.npoo, coef.ls.ind, coef.mm)
```

# Plot

## Grouped Data

```{r}
gp_data <- data_head %>%
  group_by(ID) %>%
  # filter(ID=="100693509718" | ID=="101129844643") %>%
  summarise(N=n(),
            m.propLT=mean(prop.LT, na.rm=T),
            sd.propLT=sd(prop.LT, na.rm=T)) %>%
  mutate(Nmax=as.factor(ifelse(N<=3, 3, 
                     ifelse(N>3 & N<=5, 5,
                            10)))) %>%
  inner_join(all_coef)
  
sp_gp_data <- gp_data %>%
  group_by(Method) %>%
  summarise(N=n(),
            m.propLT=mean(m.propLT, na.rm=T),
            sd.propLT=sd(sd.propLT, na.rm=T),
            m.int=mean(Intercept, na.rm=T))
```

## Filling colors

```{r}
library(RColorBrewer)
set.seed(88)
n <- length(unique(data_head$ID))
n
fill <- grDevices::colors()
fill1 <- sample(fill, n)
```

In the below plots, we visualise the individual coefficients, corresponding to the sum of the fixed effect and the individual variance component (as specified in the random effect). The error bars represent the standard errors, calculated as the the joint MSE of the summated variances of the fixed, and the random effect for the mixed model. 

These plots nicely illustrates how standard errors of the individual coefficients of the DV increases for participants with fewer observations. This is the core of the essential difference between standard regression and mixed models: thanks to the random effect, the mixed model is able to treat coefficients at the individual level, thus weighting them for their reliability (eg, standard error and sample size) when calculating the fixed betas.

In the first plot, we observe that the intercept estimated by the first two regressions is shifted to the more extreme values that, at the same time, are also the less reliable, as it is evident from plot 2 and 3. The estimate of the intercept instead is more centered because unreliable data points influence less the estimate of the fixed beta. 

In this kind of scenario, it is highly advisable to use mixed modelling, to avoid over/down-estimating a crucial estimate in a regression. While this is a strong argument, the usual condition of "having lots of missing data" might not sound as strong!

## On mean

```{r, message=FALSE, error=FALSE}
library(ggplot2)
comp_m <- ggplot(data=(gp_data),
       aes(x=m.propLT, y=Intercept, 
           fill=ID, shape=Method)) +
  facet_grid(~Method) +
  
  geom_errorbar(aes(x=m.propLT,
                    ymin=Intercept-St.er.int,
                    ymax=Intercept+St.er.int),
                position=position_dodge(2),
                color="black") +
  
  geom_jitter(aes(alpha=N), size=7) +
  
  scale_shape_manual(values = c(21,22,23)) +
  
  scale_fill_manual(values=fill1) + 
  
  guides(fill = "none", size="none") +
  
  xlim(0,1) +
  
  # scale_size_continuous(range = c(5,15)) +
  
  geom_hline(yintercept = sp_gp_data$m.int[1]) + 
  geom_hline(yintercept = sp_gp_data$m.int[2], linetype="dotted") + 
  geom_hline(yintercept = sp_gp_data$m.int[3], linetype="dashed") +
  
  labs(caption="___ = non-pooled, \n . . . = individual \n _ _ _ = mixed") +
  theme_bw()
```

```{r, fi.align="center", fig.width = 12, fig.height = 9, dpi=300}
comp_m
# ggsave(filename = "method-com-m.pdf", plot = comp_m, height = 9, width = 12)
```

## On N

```{r}
comp_n1 <- ggplot(data=gp_data,
       aes(x=N, y=Intercept, 
           fill=ID, shape=Method)) +
  geom_errorbar(aes(ymin=Intercept-St.er.int,
                    ymax=Intercept+St.er.int),
                position=position_dodge(0.5),
                color="black") +
  geom_point(aes(size=N), position=position_dodge(0.5)) +
  
  scale_shape_manual(values = c(21,22,23)) +
  
  scale_fill_manual(values=fill1) + 
  
  guides(fill = "none", size="none") +
  
  scale_size_continuous(range = c(5,15)) +
  
  geom_hline(yintercept = sp_gp_data$m.int[1]) + 
  geom_hline(yintercept = sp_gp_data$m.int[2], linetype="dotted") + 
  geom_hline(yintercept = sp_gp_data$m.int[3], linetype="dashed") +
  
  labs(caption="___ = non-pooled, \n . . . = individual \n _ _ _ = mixed") +
  
  geom_text(aes(x = 0, y = 0.38, label = "- avg"), 
             size=2.5, col="red", hjust = 0) +
  geom_text(aes(x = 0, y = 0.38+0.05, label = "- CI 97.5%"), 
             size = 2.5, col="orangered", hjust = 0) +
  geom_text(aes(x = 0, y = 0.38-0.05, label = "- CI 2.5%"), 
             size = 2.5, col="orangered", hjust = 0) +
  theme_bw()
```

```{r, fi.align="center", fig.width = 12, fig.height = 9, dpi=300}
comp_n1
# ggsave(filename = "method-com-n.pdf", plot = comp_n, height = 7, width = 11)
```

```{r}
comp_n2 <- ggplot(data=gp_data,
       aes(x=N, y=Intercept, 
           fill=ID, shape=Method)) +
  geom_errorbar(aes(ymin=Intercept-St.er.int,
                    ymax=Intercept+St.er.int),
                position=position_dodge(0.5),
                color="black") +
  geom_point(aes(size=N), position=position_dodge(0.5)) +
  
  scale_shape_manual(values = c(21,22,23)) +
  
  scale_fill_manual(values=fill1) + 
  
  guides(fill = "none", size="none") +
  
  scale_size_continuous(range = c(5,15)) +
  
  geom_hline(yintercept = sp_gp_data$m.int[1]) + 
  geom_hline(yintercept = sp_gp_data$m.int[2], linetype="dotted") + 
  geom_hline(yintercept = sp_gp_data$m.int[3], linetype="dashed") +
  
  labs(caption="___ = non-pooled, \n . . . = individual \n _ _ _ = mixed") +
  theme_bw() +
  facet_grid(~Method)
```

```{r, fi.align="center", fig.width = 12, fig.height = 9, dpi=300}
comp_n2
# ggsave(filename = "method-com-n.pdf", plot = comp_n, height = 7, width = 11)
```
